{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ AlphaTraderLab v0 - Random Agent Demo\n",
    "\n",
    "---\n",
    "\n",
    "## Welcome to AlphaTraderLab!\n",
    "\n",
    "In this notebook, we'll:\n",
    "1. Set up the trading environment\n",
    "2. Download historical Bitcoin price data\n",
    "3. Test the environment with a **random agent** (no learning yet!)\n",
    "4. Visualize the results\n",
    "\n",
    "**What is a Random Agent?**  \n",
    "It's an agent that randomly chooses actions (FLAT, LONG, or SHORT) without any intelligence. Think of it as flipping a coin to decide what to trade. Of course, it will perform poorly‚Äîbut it's a great way to test that our environment works correctly!\n",
    "\n",
    "**In Step 2**, we'll train a smart RL agent that actually learns from data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Setup and Installation\n",
    "\n",
    "First, we need to install all required packages. This cell detects if we're running in Google Colab and installs dependencies if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we're running in Google Colab\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üåê Running in Google Colab - Installing dependencies...\")\n",
    "    \n",
    "    # Install required packages\n",
    "    !pip install -q numpy pandas matplotlib yfinance gymnasium stable-baselines3 scipy\n",
    "    \n",
    "    print(\"‚úÖ Installation complete!\")\n",
    "else:\n",
    "    print(\"üíª Running locally - Make sure you've installed requirements.txt\")\n",
    "    print(\"   Run: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Step 2: Import Libraries\n",
    "\n",
    "Now let's import all the Python libraries we need:\n",
    "- **numpy**: For numerical operations\n",
    "- **pandas**: For handling data (price tables)\n",
    "- **matplotlib**: For creating charts\n",
    "- **yfinance**: For downloading historical market data\n",
    "- **gymnasium**: The RL environment framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import gymnasium as gym\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 3: Load the Trading Environment\n",
    "\n",
    "If we're in Colab, we need to upload our custom trading environment code.  \n",
    "If we're running locally, we can just import it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"üìÅ In Colab: Please upload the trading_env.py file\")\n",
    "    print(\"   (You can find it in the 'envs' folder of the project)\")\n",
    "    print()\n",
    "    \n",
    "    from google.colab import files\n",
    "    \n",
    "    # Upload the trading environment file\n",
    "    print(\"üëâ Click 'Choose Files' and select 'trading_env.py'\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    print(\"\\n‚úÖ File uploaded!\")\n",
    "    \n",
    "    # Import the TradingEnv class\n",
    "    from trading_env import TradingEnv\n",
    "    \n",
    "else:\n",
    "    print(\"üíª Running locally - Importing TradingEnv from local files\")\n",
    "    \n",
    "    # Add parent directory to path so we can import from envs\n",
    "    import os\n",
    "    import sys\n",
    "    \n",
    "    # Go up one level from notebooks/ to reach the project root\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "    \n",
    "    # Now import from the envs package\n",
    "    from envs.trading_env import TradingEnv\n",
    "\n",
    "print(\"‚úÖ TradingEnv loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 4: Download Historical Market Data\n",
    "\n",
    "Let's download Bitcoin (BTC-USD) historical price data from Yahoo Finance.  \n",
    "We'll get **daily candles** from 2018 to today.\n",
    "\n",
    "**What is OHLCV data?**\n",
    "- **O**pen: Price at the start of the day\n",
    "- **H**igh: Highest price during the day\n",
    "- **L**ow: Lowest price during the day\n",
    "- **C**lose: Price at the end of the day\n",
    "- **V**olume: How much was traded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì° Downloading BTC-USD historical data from Yahoo Finance...\")\n",
    "print(\"   This may take a few seconds...\")\n",
    "print()\n",
    "\n",
    "# Download Bitcoin data (daily candles)\n",
    "ticker = \"BTC-USD\"\n",
    "start_date = \"2018-01-01\"\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Fetch the data\n",
    "df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\", progress=False)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"‚úÖ Downloaded {len(df)} days of {ticker} data\")\n",
    "print(f\"   Date range: {df.index[0].strftime('%Y-%m-%d')} to {df.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print()\n",
    "print(\"üìã First 5 rows of the data:\")\n",
    "print(df.head())\n",
    "print()\n",
    "print(\"üìã Last 5 rows of the data:\")\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Step 5: Visualize the Price Data\n",
    "\n",
    "Let's plot the Bitcoin closing price over time to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a nice size\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot the closing price\n",
    "plt.plot(df.index, df['Close'], label='BTC-USD Close Price', color='#FF6B35', linewidth=2)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('üìä Bitcoin (BTC-USD) Historical Price', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Price (USD)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "print(f\"üí∞ Current BTC price: ${df['Close'].iloc[-1]:,.2f}\")\n",
    "print(f\"üìà All-time high in this dataset: ${df['Close'].max():,.2f}\")\n",
    "print(f\"üìâ All-time low in this dataset: ${df['Close'].min():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Step 6: Create the Trading Environment\n",
    "\n",
    "Now let's create an instance of our `TradingEnv` with the Bitcoin data we just downloaded.\n",
    "\n",
    "We'll configure it with:\n",
    "- **window_size = 30**: The agent sees the last 30 days of price data\n",
    "- **initial_balance = $10,000**: Starting with $10k\n",
    "- **transaction_cost = 0.1%**: Small fee when we change positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéÆ Creating the trading environment...\")\n",
    "\n",
    "# Create the environment\n",
    "env = TradingEnv(\n",
    "    df=df,\n",
    "    window_size=30,          # Agent sees 30 days of history\n",
    "    initial_balance=10000.0, # Start with $10,000\n",
    "    transaction_cost=0.001   # 0.1% fee per trade\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Environment created successfully!\")\n",
    "print()\n",
    "print(\"üìä Environment Details:\")\n",
    "print(f\"   - Observation space: {env.observation_space}\")\n",
    "print(f\"   - Action space: {env.action_space}\")\n",
    "print(f\"   - Actions: 0=FLAT, 1=LONG, 2=SHORT\")\n",
    "print(f\"   - Data length: {len(df)} days\")\n",
    "print(f\"   - Window size: {env.window_size} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ Step 7: Test with a Random Agent\n",
    "\n",
    "Let's run a simple test: we'll create an agent that takes **random actions** and see how it performs.\n",
    "\n",
    "This is just to verify that our environment works correctly. The random agent will:\n",
    "1. Reset the environment\n",
    "2. Take 200 random steps\n",
    "3. Track the portfolio value at each step\n",
    "\n",
    "**Expected result**: The agent will probably lose money (that's normal for random trading!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé≤ Running a random agent for 200 steps...\")\n",
    "print(\"   (This agent randomly chooses: FLAT, LONG, or SHORT)\")\n",
    "print()\n",
    "\n",
    "# Reset the environment\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Storage for tracking performance\n",
    "equity_history = [env.initial_balance]  # Start with initial balance\n",
    "action_history = []\n",
    "reward_history = []\n",
    "\n",
    "# Run for 200 steps (or until done)\n",
    "num_steps = 200\n",
    "done = False\n",
    "\n",
    "for step in range(num_steps):\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()  # Randomly choose 0, 1, or 2\n",
    "    \n",
    "    # Execute the action in the environment\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # Store the results\n",
    "    equity_history.append(info['equity'])\n",
    "    action_history.append(action)\n",
    "    reward_history.append(reward)\n",
    "    \n",
    "    # Print progress every 50 steps\n",
    "    if (step + 1) % 50 == 0:\n",
    "        action_names = ['FLAT', 'LONG', 'SHORT']\n",
    "        print(f\"   Step {step + 1:3d}: Action={action_names[action]}, \"\n",
    "              f\"Equity=${info['equity']:,.2f}, Reward={reward:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ Random agent test complete!\")\n",
    "print()\n",
    "print(\"üìä Final Results:\")\n",
    "print(f\"   - Starting balance: ${env.initial_balance:,.2f}\")\n",
    "print(f\"   - Final equity: ${equity_history[-1]:,.2f}\")\n",
    "print(f\"   - Total return: {((equity_history[-1] / env.initial_balance) - 1) * 100:.2f}%\")\n",
    "print(f\"   - Total steps: {len(equity_history) - 1}\")\n",
    "print(f\"   - Episode ended: {'Yes' if done else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 8: Visualize Agent Performance\n",
    "\n",
    "Let's create some charts to see how the random agent performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with multiple subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Subplot 1: Equity Curve (Portfolio Value Over Time)\n",
    "axes[0].plot(equity_history, color='#2E86AB', linewidth=2)\n",
    "axes[0].axhline(y=env.initial_balance, color='gray', linestyle='--', label='Initial Balance')\n",
    "axes[0].set_title('üí∞ Portfolio Equity Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Equity (USD)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Actions Taken\n",
    "action_colors = ['#A8DADC', '#457B9D', '#E63946']  # FLAT=blue, LONG=darker blue, SHORT=red\n",
    "action_names = ['FLAT', 'LONG', 'SHORT']\n",
    "for i in range(3):\n",
    "    action_mask = [1 if a == i else 0 for a in action_history]\n",
    "    axes[1].scatter(\n",
    "        range(len(action_history)), \n",
    "        action_mask,\n",
    "        c=action_colors[i], \n",
    "        label=action_names[i],\n",
    "        alpha=0.6,\n",
    "        s=20\n",
    "    )\n",
    "axes[1].set_title('üéØ Actions Taken (0=FLAT, 1=LONG, 2=SHORT)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Action')\n",
    "axes[1].set_yticks([0, 1, 2])\n",
    "axes[1].set_yticklabels(['FLAT', 'LONG', 'SHORT'])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Rewards Over Time\n",
    "axes[2].plot(reward_history, color='#06A77D', linewidth=1, alpha=0.7)\n",
    "axes[2].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2].set_title('üìà Rewards per Step', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].set_ylabel('Reward')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Adjust layout and show\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print action distribution\n",
    "print(\"\\nüìä Action Distribution:\")\n",
    "action_counts = pd.Series(action_history).value_counts().sort_index()\n",
    "for action_idx, count in action_counts.items():\n",
    "    action_name = ['FLAT', 'LONG', 'SHORT'][action_idx]\n",
    "    percentage = (count / len(action_history)) * 100\n",
    "    print(f\"   {action_name}: {count} times ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 9: Analyze the Environment\n",
    "\n",
    "Let's take a closer look at what the agent actually \"sees\" (the observation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment to get a fresh observation\n",
    "observation, info = env.reset(seed=123)\n",
    "\n",
    "print(\"üîç Analyzing the Observation Space\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(f\"üìê Observation shape: {observation.shape}\")\n",
    "print(f\"   Total features: {len(observation)}\")\n",
    "print()\n",
    "print(\"üß© Observation breakdown:\")\n",
    "window_features = env.window_size * 5  # OHLCV = 5 features per candle\n",
    "print(f\"   - Market data (OHLCV): {window_features} values\")\n",
    "print(f\"     ({env.window_size} candles √ó 5 features)\")\n",
    "print(f\"   - Current position: 1 value\")\n",
    "print(f\"   - Equity ratio: 1 value\")\n",
    "print()\n",
    "print(\"üìä Sample observation values (first 10 features):\")\n",
    "print(f\"   {observation[:10]}\")\n",
    "print()\n",
    "print(\"üìä Sample observation values (last 5 features):\")\n",
    "print(f\"   {observation[-5:]}\")\n",
    "print()\n",
    "print(\"‚úÖ The observation is a flat vector that the RL agent will learn from!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 10: Understanding the Results\n",
    "\n",
    "### What Did We Learn?\n",
    "\n",
    "1. **The Environment Works!** ‚úÖ  \n",
    "   We successfully created a trading environment that follows the Gymnasium API.\n",
    "\n",
    "2. **Random Trading is Bad** üìâ  \n",
    "   The random agent probably lost money (or made very little). This shows that smart decision-making matters!\n",
    "\n",
    "3. **Observations are Complex** üß©  \n",
    "   The agent sees 30 days of OHLCV data plus portfolio info‚Äîa lot of information to process.\n",
    "\n",
    "4. **Actions Have Consequences** ‚ö°  \n",
    "   Each action (FLAT/LONG/SHORT) affects the portfolio value, and transaction costs add up.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Step 2**, we'll:\n",
    "- Train a **PPO (Proximal Policy Optimization)** agent\n",
    "- The agent will learn to recognize profitable patterns\n",
    "- Compare its performance to the random agent\n",
    "- Add more sophisticated metrics (Sharpe ratio, max drawdown, etc.)\n",
    "\n",
    "### Important Reminders\n",
    "\n",
    "‚ö†Ô∏è **This is for learning only!**  \n",
    "Do NOT use this to trade real money without extensive testing and validation.\n",
    "\n",
    "üß† **Learning Takes Time**  \n",
    "RL agents need thousands (or millions) of steps to learn good strategies.\n",
    "\n",
    "üìö **Keep Experimenting**  \n",
    "Try different:\n",
    "- Assets (stocks, crypto, forex)\n",
    "- Window sizes\n",
    "- Reward functions\n",
    "- Transaction costs\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully set up your first RL trading environment! üöÄ\n",
    "\n",
    "Keep learning, keep experimenting, and remember: the goal is to understand the fundamentals.\n",
    "\n",
    "**See you in Step 2!** üëã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Optional: Save the Environment State\n",
    "\n",
    "If you want to save your results for later analysis, run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the equity history and action history to a CSV file\n",
    "results_df = pd.DataFrame({\n",
    "    'step': range(len(equity_history)),\n",
    "    'equity': equity_history,\n",
    "    'action': [None] + action_history,  # First row has no action\n",
    "    'reward': [None] + reward_history    # First row has no reward\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "filename = 'random_agent_results.csv'\n",
    "results_df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"üíæ Results saved to: {filename}\")\n",
    "print(f\"   You can download this file and analyze it later.\")\n",
    "\n",
    "# If in Colab, download the file\n",
    "if IN_COLAB:\n",
    "    files.download(filename)\n",
    "    print(f\"üì• File '{filename}' downloaded to your computer!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
