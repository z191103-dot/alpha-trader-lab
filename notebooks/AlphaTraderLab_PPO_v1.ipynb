{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– AlphaTraderLab PPO v1 - Training & Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## Welcome to Step 2!\n",
    "\n",
    "In **Step 1**, we built a trading environment and tested it with a random agent.\n",
    "\n",
    "In **Step 2** (this notebook), we'll:\n",
    "1. **Train** a smart RL agent using PPO (Proximal Policy Optimization)\n",
    "2. **Evaluate** it against baselines (random agent, buy & hold)\n",
    "3. **Visualize** and compare the results\n",
    "\n",
    "### ğŸ§  What is PPO?\n",
    "\n",
    "**PPO (Proximal Policy Optimization)** is a state-of-the-art reinforcement learning algorithm that:\n",
    "- Learns by trying different actions and seeing what works\n",
    "- Uses a neural network to map observations (market data) â†’ actions (FLAT/LONG/SHORT)\n",
    "- Updates the network gradually to improve trading decisions\n",
    "- Is stable, reliable, and widely used in RL research\n",
    "\n",
    "Think of it as teaching a robot trader by letting it practice thousands of times, learning from its mistakes.\n",
    "\n",
    "### ğŸ“Š What We'll Do\n",
    "\n",
    "1. **Download** Bitcoin price data (2018-present)\n",
    "2. **Split** into training (70%) and testing (30%) periods\n",
    "3. **Train** PPO agent on training data\n",
    "4. **Evaluate** on test data:\n",
    "   - PPO agent (our trained model)\n",
    "   - Random agent (baseline)\n",
    "   - Buy & Hold (benchmark)\n",
    "5. **Compare** performance with charts and tables\n",
    "\n",
    "Let's get started! ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1: Setup and Installation\n",
    "\n",
    "First, we'll install all required packages and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we're running in Google Colab\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸŒ Running in Google Colab - Installing dependencies...\")\n",
    "    \n",
    "    # Install required packages\n",
    "    !pip install -q numpy pandas matplotlib yfinance gymnasium stable-baselines3\n",
    "    \n",
    "    print(\"âœ… Installation complete!\")\n",
    "else:\n",
    "    print(\"ğŸ’» Running locally - Make sure you've installed requirements.txt\")\n",
    "    print(\"   Run: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Step 2: Import Libraries\n",
    "\n",
    "Import all necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Stable-Baselines3 imports\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 3: Load the Trading Environment\n",
    "\n",
    "We'll load our custom `TradingEnv` from Step 1.\n",
    "\n",
    "**Note**: In Colab, you'll need to upload `trading_env.py` and `evaluation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"ğŸ“ In Colab: Please upload the required files\")\n",
    "    print(\"   1. trading_env.py (from envs/ folder)\")\n",
    "    print(\"   2. evaluation.py (from utils/ folder)\")\n",
    "    print()\n",
    "    \n",
    "    from google.colab import files\n",
    "    \n",
    "    # Upload trading_env.py\n",
    "    print(\"ğŸ‘‰ Upload 'trading_env.py' first:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Upload evaluation.py\n",
    "    print(\"\\nğŸ‘‰ Upload 'evaluation.py' next:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    print(\"\\nâœ… Files uploaded!\")\n",
    "    \n",
    "    # Import from uploaded files\n",
    "    from trading_env import TradingEnv\n",
    "    from evaluation import (\n",
    "        evaluate_agent,\n",
    "        evaluate_random_agent,\n",
    "        evaluate_buy_and_hold,\n",
    "        compare_agents\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"ğŸ’» Running locally - Importing from project structure\")\n",
    "    \n",
    "    # Add parent directory to path\n",
    "    import sys\n",
    "    import os\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "    \n",
    "    # Import from project structure\n",
    "    from envs.trading_env import TradingEnv\n",
    "    from utils.evaluation import (\n",
    "        evaluate_agent,\n",
    "        evaluate_random_agent,\n",
    "        evaluate_buy_and_hold,\n",
    "        compare_agents\n",
    "    )\n",
    "\n",
    "print(\"âœ… TradingEnv and evaluation utilities loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 4: Download and Split Data\n",
    "\n",
    "We'll download Bitcoin historical data and split it into:\n",
    "- **Training set** (70%): Used to train the PPO agent\n",
    "- **Testing set** (30%): Used to evaluate all agents\n",
    "\n",
    "### ğŸ¤” Why Split?\n",
    "\n",
    "We want to see if our agent can generalize to **unseen** data. Training and testing on the same data would be cheating - the agent would just memorize the answers!\n",
    "\n",
    "Think of it like studying for an exam: you practice on some problems (training), then take a test with new problems (testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¡ Downloading BTC-USD historical data...\")\n",
    "print(\"   This may take 10-20 seconds...\")\n",
    "print()\n",
    "\n",
    "# Download Bitcoin data\n",
    "ticker = \"BTC-USD\"\n",
    "start_date = \"2018-01-01\"\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\", progress=False)\n",
    "\n",
    "print(f\"âœ… Downloaded {len(df)} days of {ticker} data\")\n",
    "print(f\"   Date range: {df.index[0].strftime('%Y-%m-%d')} to {df.index[-1].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Split into train (70%) and test (30%)\n",
    "train_ratio = 0.7\n",
    "split_index = int(len(df) * train_ratio)\n",
    "\n",
    "train_df = df.iloc[:split_index].copy()\n",
    "test_df = df.iloc[split_index:].copy()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Data Split:\")\n",
    "print(f\"   Training:   {len(train_df):4d} days ({train_df.index[0].strftime('%Y-%m-%d')} to {train_df.index[-1].strftime('%Y-%m-%d')})\")\n",
    "print(f\"   Testing:    {len(test_df):4d} days ({test_df.index[0].strftime('%Y-%m-%d')} to {test_df.index[-1].strftime('%Y-%m-%d')})\")\n",
    "\n",
    "# Show first few rows of training data\n",
    "print(\"\\nğŸ“‹ Training data preview:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 5: Visualize the Train/Test Split\n",
    "\n",
    "Let's see what the data split looks like on a chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot training data\n",
    "plt.plot(train_df.index, train_df['Close'], label='Training Data', color='#3498db', linewidth=2)\n",
    "\n",
    "# Plot testing data\n",
    "plt.plot(test_df.index, test_df['Close'], label='Testing Data', color='#e74c3c', linewidth=2)\n",
    "\n",
    "# Add vertical line at split\n",
    "plt.axvline(x=train_df.index[-1], color='gray', linestyle='--', linewidth=2, label='Train/Test Split')\n",
    "\n",
    "plt.title('ğŸ“Š BTC-USD Price: Train/Test Split', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Price (USD)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ The agent will train on the blue section and be tested on the red section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ® Step 6: Create Training and Testing Environments\n",
    "\n",
    "We'll create two environments:\n",
    "1. **Training environment**: Uses `train_df`\n",
    "2. **Testing environment**: Uses `test_df`\n",
    "\n",
    "### ğŸ”§ VecEnv Wrapper\n",
    "\n",
    "Stable-Baselines3 requires environments to be \"vectorized\" (wrapped in `DummyVecEnv`). This allows for efficient batched computation, even though we're only using one environment.\n",
    "\n",
    "Think of it as putting your environment in a special container that SB3 knows how to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "WINDOW_SIZE = 30\n",
    "INITIAL_BALANCE = 10000.0\n",
    "TRANSACTION_COST = 0.001  # 0.1%\n",
    "\n",
    "print(\"ğŸ® Creating environments...\")\n",
    "print(f\"   Window size: {WINDOW_SIZE} days\")\n",
    "print(f\"   Initial balance: ${INITIAL_BALANCE:,.2f}\")\n",
    "print(f\"   Transaction cost: {TRANSACTION_COST*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Helper function to create a vectorized environment\n",
    "def make_env(df, window_size=WINDOW_SIZE, initial_balance=INITIAL_BALANCE):\n",
    "    \"\"\"\n",
    "    Create a TradingEnv and wrap it in DummyVecEnv for Stable-Baselines3.\n",
    "    \n",
    "    This wrapper is required by SB3 for all environments.\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        return TradingEnv(\n",
    "            df=df,\n",
    "            window_size=window_size,\n",
    "            initial_balance=initial_balance,\n",
    "            transaction_cost=TRANSACTION_COST\n",
    "        )\n",
    "    return DummyVecEnv([_init])\n",
    "\n",
    "# Create training environment (vectorized)\n",
    "train_env = make_env(train_df)\n",
    "print(\"âœ… Training environment created (vectorized for SB3)\")\n",
    "\n",
    "# Create testing environment (non-vectorized for easier evaluation)\n",
    "test_env = TradingEnv(\n",
    "    df=test_df,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    initial_balance=INITIAL_BALANCE,\n",
    "    transaction_cost=TRANSACTION_COST\n",
    ")\n",
    "print(\"âœ… Testing environment created\")\n",
    "\n",
    "# Show environment info\n",
    "print(f\"\\nğŸ“Š Environment details:\")\n",
    "print(f\"   Observation space: {test_env.observation_space.shape}\")\n",
    "print(f\"   Action space: {test_env.action_space} (0=FLAT, 1=LONG, 2=SHORT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 7: Train the PPO Agent\n",
    "\n",
    "Now comes the exciting part - training our RL agent!\n",
    "\n",
    "### ğŸ§  How PPO Learns\n",
    "\n",
    "1. **Observe**: Look at market data (30 days of OHLCV)\n",
    "2. **Act**: Choose an action (FLAT, LONG, or SHORT)\n",
    "3. **Reward**: Get feedback (did we make or lose money?)\n",
    "4. **Learn**: Update the neural network to make better decisions\n",
    "5. **Repeat**: Do this thousands of times\n",
    "\n",
    "### â±ï¸ Training Time\n",
    "\n",
    "- **100,000 steps**: ~3-5 minutes on Colab (GPU)\n",
    "- **200,000 steps**: ~6-10 minutes (better results)\n",
    "- **50,000 steps**: ~2-3 minutes (quick test)\n",
    "\n",
    "We'll start with 100,000 steps as a good balance between time and performance.\n",
    "\n",
    "### ğŸ“š Hyperparameters Explained\n",
    "\n",
    "- **learning_rate** (3e-4): How fast the agent learns (too high = unstable, too low = slow)\n",
    "- **n_steps** (2048): Steps before each update\n",
    "- **batch_size** (64): Size of minibatches for training\n",
    "- **gamma** (0.99): How much to value future rewards\n",
    "- **ent_coef** (0.01): Encourages exploration\n",
    "\n",
    "These are standard values that work well for most problems. You can experiment later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TOTAL_TIMESTEPS = 100_000  # Reduce to 50_000 for faster testing\n",
    "\n",
    "print(\"ğŸ¤– Creating PPO agent...\")\n",
    "print()\n",
    "\n",
    "# Create PPO model\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",           # Multi-Layer Perceptron (neural network)\n",
    "    env=train_env,                # Training environment\n",
    "    learning_rate=3e-4,           # Learning rate\n",
    "    n_steps=2048,                 # Steps before update\n",
    "    batch_size=64,                # Minibatch size\n",
    "    n_epochs=10,                  # Training epochs per update\n",
    "    gamma=0.99,                   # Discount factor\n",
    "    gae_lambda=0.95,              # GAE parameter\n",
    "    clip_range=0.2,               # PPO clipping parameter\n",
    "    ent_coef=0.01,                # Entropy coefficient (exploration)\n",
    "    verbose=1                      # Print progress\n",
    ")\n",
    "\n",
    "print(\"âœ… PPO agent created with standard hyperparameters\")\n",
    "print()\n",
    "print(\"ğŸ‹ï¸ Training starting...\")\n",
    "print(f\"   Total timesteps: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"   Estimated time: ~{TOTAL_TIMESTEPS // 30000} minutes\")\n",
    "print()\n",
    "print(\"ğŸ“Š Training progress:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "\n",
    "# Save the model\n",
    "model_path = \"ppo_btc_trader.zip\"\n",
    "model.save(model_path)\n",
    "print(f\"ğŸ’¾ Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 8: Evaluate All Agents on Test Data\n",
    "\n",
    "Now let's see how well our PPO agent performs compared to:\n",
    "1. **Random Agent**: Chooses actions randomly (worst case baseline)\n",
    "2. **Buy & Hold**: Buys at the start and holds (common benchmark)\n",
    "\n",
    "### ğŸ¯ What to Expect\n",
    "\n",
    "**Good results**:\n",
    "- PPO > Random (agent learned something!)\n",
    "- PPO â‰ˆ Buy & Hold or better (competitive performance)\n",
    "\n",
    "**Bad results**:\n",
    "- PPO â‰ˆ Random (agent didn't learn)\n",
    "- PPO << Buy & Hold (agent is worse than doing nothing)\n",
    "\n",
    "Remember: Trading is HARD! Even professional traders struggle to beat buy & hold consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š Evaluating all agents on test data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate PPO Agent\n",
    "print(\"\\nğŸ¤– 1. Evaluating PPO Agent (Trained)\")\n",
    "print(\"-\" * 60)\n",
    "test_env.reset()\n",
    "ppo_results = evaluate_agent(test_env, model=model, n_episodes=1, deterministic=True)\n",
    "print(f\"   Initial equity: ${ppo_results['initial_equity']:,.2f}\")\n",
    "print(f\"   Final equity:   ${ppo_results['final_equity']:,.2f}\")\n",
    "print(f\"   Total return:   {ppo_results['total_return']:.2f}%\")\n",
    "print(f\"   Total reward:   {ppo_results['total_reward']:.4f}\")\n",
    "print(f\"   Trades made:    {ppo_results['n_trades']}\")\n",
    "\n",
    "# Evaluate Random Agent\n",
    "print(\"\\nğŸ² 2. Evaluating Random Agent (Baseline)\")\n",
    "print(\"-\" * 60)\n",
    "test_env.reset()\n",
    "random_results = evaluate_random_agent(test_env, n_episodes=1)\n",
    "print(f\"   Initial equity: ${random_results['initial_equity']:,.2f}\")\n",
    "print(f\"   Final equity:   ${random_results['final_equity']:,.2f}\")\n",
    "print(f\"   Total return:   {random_results['total_return']:.2f}%\")\n",
    "print(f\"   Total reward:   {random_results['total_reward']:.4f}\")\n",
    "print(f\"   Trades made:    {random_results['n_trades']}\")\n",
    "\n",
    "# Evaluate Buy & Hold\n",
    "print(\"\\nğŸ“ˆ 3. Evaluating Buy & Hold (Benchmark)\")\n",
    "print(\"-\" * 60)\n",
    "test_env.reset()\n",
    "bh_results = evaluate_buy_and_hold(test_env, n_episodes=1)\n",
    "print(f\"   Initial equity: ${bh_results['initial_equity']:,.2f}\")\n",
    "print(f\"   Final equity:   ${bh_results['final_equity']:,.2f}\")\n",
    "print(f\"   Total return:   {bh_results['total_return']:.2f}%\")\n",
    "print(f\"   Total reward:   {bh_results['total_reward']:.4f}\")\n",
    "print(f\"   Trades made:    {bh_results['n_trades']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 9: Comparison Table\n",
    "\n",
    "Let's create a nice summary table comparing all three agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dictionary\n",
    "results_dict = {\n",
    "    'PPO (Trained)': ppo_results,\n",
    "    'Random (Baseline)': random_results,\n",
    "    'Buy & Hold (Benchmark)': bh_results\n",
    "}\n",
    "\n",
    "# Generate comparison table\n",
    "comparison_df = compare_agents(results_dict)\n",
    "\n",
    "print(\"\\nğŸ“Š Agent Comparison Table\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Determine winner\n",
    "best_agent = comparison_df.iloc[0]['Agent']\n",
    "print(f\"\\nğŸ† Best Performer: {best_agent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 10: Visualize Equity Curves\n",
    "\n",
    "The most important chart: how does portfolio value change over time for each agent?\n",
    "\n",
    "### ğŸ“Š How to Read This Chart\n",
    "\n",
    "- **Y-axis**: Portfolio value (equity) in dollars\n",
    "- **X-axis**: Trading steps (each step = 1 day)\n",
    "- **Higher line = better performance**\n",
    "- **Smooth line = consistent strategy**\n",
    "- **Volatile line = risky strategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot PPO agent\n",
    "plt.plot(ppo_results['history']['step'], \n",
    "         ppo_results['history']['equity'],\n",
    "         label='PPO (Trained)', \n",
    "         color='#2ecc71',  # Green\n",
    "         linewidth=2.5,\n",
    "         alpha=0.9)\n",
    "\n",
    "# Plot Random agent\n",
    "plt.plot(random_results['history']['step'], \n",
    "         random_results['history']['equity'],\n",
    "         label='Random (Baseline)', \n",
    "         color='#e74c3c',  # Red\n",
    "         linewidth=2,\n",
    "         alpha=0.7,\n",
    "         linestyle='--')\n",
    "\n",
    "# Plot Buy & Hold\n",
    "plt.plot(bh_results['history']['step'], \n",
    "         bh_results['history']['equity'],\n",
    "         label='Buy & Hold (Benchmark)', \n",
    "         color='#3498db',  # Blue\n",
    "         linewidth=2,\n",
    "         alpha=0.7,\n",
    "         linestyle='-.')\n",
    "\n",
    "# Add initial balance line\n",
    "plt.axhline(y=INITIAL_BALANCE, \n",
    "            color='gray', \n",
    "            linestyle=':', \n",
    "            linewidth=1.5,\n",
    "            label=f'Initial Balance (${INITIAL_BALANCE:,.0f})',\n",
    "            alpha=0.5)\n",
    "\n",
    "# Customize plot\n",
    "plt.title('ğŸ’° Portfolio Equity Over Time (Test Period)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Step (Days)', fontsize=12)\n",
    "plt.ylabel('Portfolio Value ($)', fontsize=12)\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Interpretation:\")\n",
    "print(\"   - Green line (PPO): Our trained agent\")\n",
    "print(\"   - Red dashed line (Random): Worst-case baseline\")\n",
    "print(\"   - Blue dash-dot line (Buy & Hold): Common benchmark\")\n",
    "print(\"   - Gray dotted line: Starting point ($10,000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 11: Action Distribution Analysis\n",
    "\n",
    "Let's see what actions each agent took during the test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "action_names = ['FLAT', 'LONG', 'SHORT']\n",
    "action_colors = ['#95a5a6', '#3498db', '#e74c3c']  # Gray, Blue, Red\n",
    "\n",
    "# Plot PPO actions\n",
    "ppo_actions = ppo_results['history']['action']\n",
    "axes[0].plot(ppo_actions, linewidth=1, color='#2ecc71', alpha=0.7)\n",
    "axes[0].set_title('ğŸ¤– PPO Agent Actions', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Action')\n",
    "axes[0].set_yticks([0, 1, 2])\n",
    "axes[0].set_yticklabels(action_names)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Random actions\n",
    "random_actions = random_results['history']['action']\n",
    "axes[1].plot(random_actions, linewidth=1, color='#e74c3c', alpha=0.7)\n",
    "axes[1].set_title('ğŸ² Random Agent Actions', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Action')\n",
    "axes[1].set_yticks([0, 1, 2])\n",
    "axes[1].set_yticklabels(action_names)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Buy & Hold actions\n",
    "bh_actions = bh_results['history']['action']\n",
    "axes[2].plot(bh_actions, linewidth=1, color='#3498db', alpha=0.7)\n",
    "axes[2].set_title('ğŸ“ˆ Buy & Hold Actions', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Action')\n",
    "axes[2].set_xlabel('Step (Days)')\n",
    "axes[2].set_yticks([0, 1, 2])\n",
    "axes[2].set_yticklabels(action_names)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print action statistics\n",
    "print(\"\\nğŸ“Š Action Distribution:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for agent_name, results in results_dict.items():\n",
    "    actions = results['history']['action']\n",
    "    action_counts = pd.Series(actions).value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\n{agent_name}:\")\n",
    "    for action_idx in range(3):\n",
    "        count = action_counts.get(action_idx, 0)\n",
    "        pct = (count / len(actions)) * 100\n",
    "        print(f\"   {action_names[action_idx]:6s}: {count:4d} times ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 12: Performance Summary & Insights\n",
    "\n",
    "Let's summarize what we learned and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate rankings\n",
    "returns = {\n",
    "    'PPO': ppo_results['total_return'],\n",
    "    'Random': random_results['total_return'],\n",
    "    'Buy & Hold': bh_results['total_return']\n",
    "}\n",
    "\n",
    "sorted_agents = sorted(returns.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nğŸ† Rankings (by Total Return):\")\n",
    "for i, (agent, ret) in enumerate(sorted_agents, 1):\n",
    "    emoji = \"ğŸ¥‡\" if i == 1 else \"ğŸ¥ˆ\" if i == 2 else \"ğŸ¥‰\"\n",
    "    print(f\"   {emoji} {i}. {agent:25s}: {ret:7.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ğŸ“ WHAT DOES THIS MEAN?\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "ppo_return = ppo_results['total_return']\n",
    "random_return = random_results['total_return']\n",
    "bh_return = bh_results['total_return']\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nğŸ“ˆ PPO vs Random:\")\n",
    "if ppo_return > random_return + 5:\n",
    "    print(\"   âœ… GREAT! PPO significantly outperformed random trading.\")\n",
    "    print(\"   âœ… The agent learned meaningful trading strategies.\")\n",
    "elif ppo_return > random_return:\n",
    "    print(\"   âœ… GOOD! PPO beat random, but margin is small.\")\n",
    "    print(\"   ğŸ’¡ Try training longer or tuning hyperparameters.\")\n",
    "else:\n",
    "    print(\"   âš ï¸  WARNING! PPO didn't beat random.\")\n",
    "    print(\"   ğŸ’¡ The agent may need more training or better hyperparameters.\")\n",
    "\n",
    "print(\"\\nğŸ“Š PPO vs Buy & Hold:\")\n",
    "if ppo_return > bh_return + 5:\n",
    "    print(\"   ğŸ‰ EXCELLENT! PPO beat the buy & hold benchmark!\")\n",
    "    print(\"   ğŸ‰ This is hard to achieve - great result!\")\n",
    "elif ppo_return > bh_return:\n",
    "    print(\"   âœ… GOOD! PPO slightly beat buy & hold.\")\n",
    "    print(\"   ğŸ’¡ Results may vary - trading is inherently noisy.\")\n",
    "else:\n",
    "    print(\"   â„¹ï¸  PPO didn't beat buy & hold (this is common).\")\n",
    "    print(\"   â„¹ï¸  Buy & hold is a strong benchmark for trending markets.\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ğŸ’¡ IMPORTANT NOTES\")\n",
    "print(\"-\"*80)\n",
    "print(\"1. These results are on a SINGLE test period - not statistically robust.\")\n",
    "print(\"2. Past performance does NOT guarantee future results.\")\n",
    "print(\"3. Real trading has additional challenges (slippage, execution, psychology).\")\n",
    "print(\"4. This is an EDUCATIONAL PROJECT - not investment advice!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… STEP 2 COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 13: Next Steps & Improvements\n",
    "\n",
    "Congratulations on completing Step 2! Here are ideas for further exploration:\n",
    "\n",
    "### ğŸš€ Immediate Improvements\n",
    "1. **Train longer**: Increase `TOTAL_TIMESTEPS` to 200,000 or 500,000\n",
    "2. **Multiple runs**: Train several times and average results\n",
    "3. **Different splits**: Try 80/20 or 60/40 train/test split\n",
    "\n",
    "### ğŸ§ª Experiments\n",
    "1. **Other assets**: Try ETH-USD, stocks (AAPL, TSLA), etc.\n",
    "2. **Window sizes**: Try 10, 20, 50, or 100 day windows\n",
    "3. **Hyperparameters**: Adjust learning rate, entropy coefficient, etc.\n",
    "4. **Other algorithms**: Try A2C, DQN, or SAC from Stable-Baselines3\n",
    "\n",
    "### ğŸ¨ Advanced Features (Step 3?)\n",
    "1. **Technical indicators**: Add RSI, MACD, Bollinger Bands to observations\n",
    "2. **Risk metrics**: Calculate Sharpe ratio, max drawdown, etc.\n",
    "3. **Multiple timeframes**: Combine daily + hourly data\n",
    "4. **Portfolio optimization**: Trade multiple assets simultaneously\n",
    "5. **Live paper trading**: Test on real-time data (without real money)\n",
    "\n",
    "### ğŸ“š Learning Resources\n",
    "- [Stable-Baselines3 Docs](https://stable-baselines3.readthedocs.io/)\n",
    "- [PPO Paper](https://arxiv.org/abs/1707.06347) (for the curious)\n",
    "- [Spinning Up in Deep RL](https://spinningup.openai.com/) (comprehensive RL guide)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "You've successfully:\n",
    "- âœ… Trained a PPO agent from scratch\n",
    "- âœ… Evaluated it against baselines\n",
    "- âœ… Visualized and interpreted the results\n",
    "- âœ… Learned about RL in a practical context\n",
    "\n",
    "**Remember**: This is for LEARNING, not for making investment decisions!\n",
    "\n",
    "Keep experimenting, keep learning, and have fun! ğŸš€ğŸ“ˆğŸ¤–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¾ Optional: Save Results\n",
    "\n",
    "Save your results to CSV files for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save equity histories\n",
    "ppo_results['history'].to_csv('ppo_results.csv', index=False)\n",
    "random_results['history'].to_csv('random_results.csv', index=False)\n",
    "bh_results['history'].to_csv('bh_results.csv', index=False)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv('agent_comparison.csv', index=False)\n",
    "\n",
    "print(\"ğŸ’¾ Results saved:\")\n",
    "print(\"   - ppo_results.csv\")\n",
    "print(\"   - random_results.csv\")\n",
    "print(\"   - bh_results.csv\")\n",
    "print(\"   - agent_comparison.csv\")\n",
    "\n",
    "# Download files in Colab\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download('ppo_results.csv')\n",
    "    files.download('agent_comparison.csv')\n",
    "    files.download(model_path)\n",
    "    print(\"\\nğŸ“¥ Files downloaded to your computer!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
